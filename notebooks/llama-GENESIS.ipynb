{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.213647675Z",
     "start_time": "2023-08-23T09:24:03.154996176Z"
    }
   },
   "outputs": [],
   "source": [
    "# python src/transformers/models/llama/convert_llama_weights_to_hf.py     --input_dir /media/fleeb/TOWER/llama2/70B --model_size 70B --output_dir /media/fleeb/TOWER/llama2/hf/70b-c/"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# \n",
    "# client = OpenAI()\n",
    "# def ask_GPT4(prompt):\n",
    "#     completion = client.chat.completions.create(\n",
    "#       model = \"gpt-4\",\n",
    "#       messages = [ {\"role\": \"user\", \"content\": prompt} ]\n",
    "#     )\n",
    "#     return completion.choices[0].message.content\n",
    "# \n",
    "# ask_GPT4(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfbf0e48557db992"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfad429ccc5348ebab89fcfcf476ad7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/media/fleeb/TOWER/llama2/hf/7b-c/\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"/media/fleeb/TOWER/llama2/hf/7b-c/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:53:06.207817156Z",
     "start_time": "2024-01-14T15:50:16.362428668Z"
    }
   },
   "id": "50c4a3d1e9db80a",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T16:31:52.771025887Z",
     "start_time": "2024-01-14T16:31:52.725734817Z"
    }
   },
   "id": "a8148199992ff98f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T16:32:13.554313483Z",
     "start_time": "2024-01-14T16:31:53.776287776Z"
    }
   },
   "id": "637bcbfa23dd4e69",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"Hey, are you conscious? Can you talk to me?\\n\\nI'm an artificial intelligence, and I've been programmed to\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T16:32:13.554978465Z",
     "start_time": "2024-01-14T16:32:13.554156628Z"
    }
   },
   "id": "11b6db39b6a079f1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81b07ff2001849e1bfaa97909e422518"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dab3b6c0f6d14cff908f981adc5d9e91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f85445b699a4a729a5bddb975ea3060"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf06e6ee1a4b458bb605f61faa5236da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cd2dcf63acf49029fe59971b52e82f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7149421c65d7445f985f1e74dc297170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c0156cefcc3415486a2ed90501c93a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fc8dc178d214287ac40e271251617a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "571f820926ff4f828ea2784042c9c95d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d8f5924d6dc42c7b113206e7d423ae2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40f9dc1b10ae4fea94a7db8484289caf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3bff8eb5ecf430f8f0b64dcf759050f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288596898d164591a84bb3e9cfb94ceb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de7896130a6d405d9b105ad23c2d782d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5a9b0f1ea814f928846eb4b6ab7e9e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebe3a53466b845e9b506a32af77371c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b5229ad27694b1089296cb45ba152ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65fb0265e67d4cef9fc4e2d847f62b55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f4cfea9f01048d8851d1986e15b9698"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfffb3dd2fb54c52ad9a640a99ae9a1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13b6a27e3f3d47019c1e1888592dc2c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1967a35528064720b94ac9bad68a360b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a2a472e455c4d5da3a4e5ee164cd7df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "device = 'cpu'\n",
    "\n",
    "# model_id = 'Mistral-7B-v0.1'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "model_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "model_id = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-14T16:56:17.298770183Z"
    }
   },
   "id": "cf0b864e22e76b1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# python src/transformers/models/llama/convert_llama_weights_to_hf.py     --input_dir /media/fleeb/TOWER/llama2/70B --model_size 70B --output_dir /media/fleeb/TOWER/llama2/hf/70b-c/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "697a004f3e7d3f78"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bd3e68ebd7b4ba68653f55f9f2ebb1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers import pipeline\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, BitsAndBytesConfig , CodeGenTokenizer \n",
    "# from langchain.llms import HuggingFacePipeline \n",
    "# from langchain import PromptTemplate, LLMChain\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "import torch \n",
    "# torch.set_default_device(\"cuda\")\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "# model_id = 'microsoft/phi-2'\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map='cuda', load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, device_map='cuda')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T15:35:28.269221529Z",
     "start_time": "2024-01-15T15:26:25.491955755Z"
    }
   },
   "id": "263167ccc2275e20",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence, a neural network, a machine learning model. I don't have feelings, emotions, or consciousness. I don't eat, sleep, or breathe. I don't have a physical body.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "with torch.no_grad():\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T16:00:29.082244924Z",
     "start_time": "2024-01-15T16:00:25.874619938Z"
    }
   },
   "id": "9990d62aa04e4df9",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# pipe.model.config.pad_token_id = pipe.model.config.eos_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T16:01:00.866214647Z",
     "start_time": "2024-01-15T16:01:00.824164838Z"
    }
   },
   "id": "c702660bce2f46ae",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=10) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey!  How are you able.  I'm having a hard time with this.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, StoppingCriteriaList, MaxTimeCriteria\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "# generator = pipeline(\"text-generation\")\n",
    "generator = pipe\n",
    "\n",
    "# Define the stopping criteria using MaxTimeCriteria\n",
    "stopping_criteria = StoppingCriteriaList([MaxTimeCriteria(32)])\n",
    "\n",
    "# Define the generation_kwargs with stopping criteria\n",
    "generation_kwargs = {\n",
    "    \"max_length\": 100,  # Maximum length of the generated text\n",
    "    \"max_new_tokens\": 10,  # Maximum number of new tokens to generate\n",
    "    \"generation_kwargs\": {\"stopping_criteria\": stopping_criteria}  # Add stopping criteria to generation_kwargs\n",
    "}\n",
    "\n",
    "# Pass the generation_kwargs to the pipeline\n",
    "generated_text = generator(\n",
    "    \"Hey!  How are you able.\",\n",
    "    **generation_kwargs\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T14:53:58.806325380Z",
     "start_time": "2024-01-15T14:53:58.139172548Z"
    }
   },
   "id": "2e5354f4cf5be22d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 523, 28766, 321, 28730, 416, 28766, 28767]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<|im_end|>')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T16:01:08.884937194Z",
     "start_time": "2024-01-15T16:01:08.834949619Z"
    }
   },
   "id": "1dae811cf530f33d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = '''We have a news article headline, and we would like to identify a potential specific analysis that could be motivated by the article. Importantly, we would like specific details to be mentioned in the setting, even if they don't show up in the original article, to paint a vivid picture of the motivation and potential features of the data.\n",
    "\n",
    "Here is the article headline:\n",
    "\n",
    "```\n",
    "Title: Technological innovations that we will see in 2021 - 15 min\n",
    "Description: Artificial intelligence, virtual and augmented reality, the Internet of Things, robotics, automation... For almost five years, reviews of future technologies have been starting...\n",
    "Original Language: Lithuanian\n",
    "```\n",
    "\n",
    "Write a 2-3 sentence introduction to a statistical analysis that could be motivated by this article. Be sure to include specific details about the data that would be used and the analysis that would be performed. You do not need to write the entire analysis, just the introduction. Avoid using any technical jargon (such as words like \"statistical analysis\" or \"model\" and avoid starting with \"imagine\") and make sure the introduction is accessible to laypeople (albeit in a professional setting), and the motivation is clear. Imagine a casual conversation between friends or colleagues about the article that may prompt some deeper quantitative analysis when thinking about it, and make sure this introduction would sound understandable and interesting to them.\n",
    "\n",
    "Answer in the form of a json dictionary with two entries: one for the `introduction` and one for a proposed `title` for this specific setting that can serve as a file name.\n",
    "\n",
    "'''\n",
    "\n",
    "text = '''We are interested in building a causal model based on the explicit and implicit assumptions contained in the specified article and then using causal inference to evaluate the reasoning. Answer the following questions to design an interesting, simple, and most importantly realistic causal model from a news article.\n",
    "\n",
    "To help understand the instructions here are some tips:\n",
    "- all variables are always binary and (at least in principle) measurable, so when creating and selecting variables, make sure it is reasonable to treat them as binary\n",
    "- whenever you propose a variable, make sure to define the meaning of each value it can take, and mention whether it is observable or not\n",
    "- outcome variables are always observable, and should always have 2-3 causal parents (including treatment, mediator, and confounder variables)\n",
    "- treatment variables are always observable and intervenable, that means it must be possible to (at least in principle) change their value if desired, and these should generally have at least causal parent, and always at least one child\n",
    "- confounder variables may or may not be observable, and should always have 2-3 causal children and no causal parents\n",
    "- mediator variables may or may not be observable, and should always have 1-2 causal parents (for example, a treatment variable or confounder) and 1-2 causal children (for example, the outcome variable)\n",
    "- collider variables are always observable, and should always have 2-3 causal parents (for example, a treatment and outcome variable) and no causal children\n",
    "\n",
    "Here is the topic and original the news article headline:\n",
    "\n",
    "```\n",
    "Title: Technological innovations that we will see in 2021 - 15 min\n",
    "Description: Artificial intelligence, virtual and augmented reality, the Internet of Things, robotics, automation... For almost five years, reviews of future technologies have been starting...\n",
    "Original Language: Lithuanian\n",
    "\n",
    "As we dive into the era of technological advancements, it's fascinating to consider how these innovations have evolved over the years. By gathering data on the development and adoption rates of artificial intelligence, virtual reality, and the Internet of Things, we can create a vivid picture of their growth trends and future potential. This exploration will highlight which technologies are becoming integral parts of our daily lives and how rapidly they are being embraced worldwide.\n",
    "```\n",
    "\n",
    "Begin by brainstorming some non-trivial interesting binary causal variables to construct a causal bayes net:\n",
    "\n",
    "1. Propose 1 outcome variable mentioned implicitly or explicitly in the introduction addressing some quantity that people are most likely to be interested in studying, especially if people tend to have misconceptions about it\n",
    "3. Propose 2 treatment variables that either directly or indirectly affect the selected outcome variable and are the most interesting to study. Make sure the treatment variables affect the outcome (possibly through a mediator).\n",
    "4. Propose 2 confounder variables that affect some reasonable combination of the outcome, treatment, and mediator variables in a non-trivial way. Make sure the confounders have at least one child.\n",
    "5. Propose 2 mediator variables that affect and are affected by some reasonable combination of any other variables in a non-trivial way. Make sure the mediators have both parents and children.\n",
    "6. Propose 1 collider variable that are affected by some reasonable combination of any other variables in a non-trivial way. Make sure the collider has at least two parents.\n",
    "\n",
    "The variables and causal graph should, where possible, use specific details such as names and locations mentioned in the article. Also, generally the variable value \"0\" should correspond to the negative, neutral, or control while the value \"1\" should correspond to the positive choice or active value.\n",
    "\n",
    "As part of the brainstorming you should briefly first list the names of all the proposed variables.\n",
    "\n",
    "Next, construct a causal graph using the proposed variables by listing all the edges in the graph. Make sure to into the outcome variable and both treatment variables, and some interesting and intuitive combination of the other variables (you don't have to use all the others). Important: Make sure the causal graph is a DAG and that each node has at most THREE parents!\n",
    "\n",
    "Formalize this causal graph in the form of a JSON list of variables where each variable contains the following fields:\n",
    "- `name`: the name of the variable\n",
    "- `description`: a short description of the variable\n",
    "- `type`: the type of the variable, which can be one of the following: `outcome`, `treatment`, `confounder`, `mediator`, `collider`\n",
    "- `observed`: a boolean value indicating whether the variable is observable or not\n",
    "- `values`: a JSON list of the descriptions of the values the variable can take (corresponding to the index)\n",
    "- `parents`: a JSON list of the names of the parents of the variable (make sure they match the corresponding `name` field of the parent nodes, and remember, there should not be more than three parents for any node)\n",
    "\n",
    "Aside from some brief brainstorming, answer concisely and precisely with a JSON list in the desired format.\n",
    "\n",
    "Take a deep breath and think step-by-step how you are going to do this.'''\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    # {\n",
    "    #     \"role\": \"system\",\n",
    "    #     \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    # },\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T16:12:39.955496534Z",
     "start_time": "2024-01-15T16:12:39.919741164Z"
    }
   },
   "id": "8d3e5f1a776dea40",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided news article, I suggest the following causal variables and their relationships:\n",
      "1. Outcome Variable:\n",
      "  * Name: TechnologyAdoptionRate\n",
      "  * Description: The rate at which new technology is adopted globally.\n",
      "  * Type: outcome\n",
      "  * Observed: Yes\n",
      "  * Values: [\"Low\", \"Medium\", \"High\"]\n",
      "  * Parents: []\n",
      "2. Treatment Variables:\n",
      "  a. Name: AIInvestment\n",
      "     * Description: Investment in artificial intelligence research and implementation.\n",
      "     * Type: treatment\n",
      "     * Observed: Yes\n",
      "     * Values: [\"Low\", \"High\"]\n",
      "     * Parents: []\n",
      "  b. Name: IoTInfrastructureDevelopment\n",
      "     * Description: Development and expansion of Internet of Things infrastructure.\n",
      "     * Type: treatment\n",
      "     * Observed: Yes\n",
      "     * Values: [\"Limited\", \"Expanded\"]\n",
      "     * Parents: []\n",
      "3. Confounder Variables:\n",
      "  a. Name: EconomicConditions\n",
      "     * Description: General economic conditions influencing investment decisions and consumer behavior.\n",
      "     * Type: confounder\n",
      "     * Observed: Yes\n",
      "     * Values: [\"Recessionary\", \"Prosperous\"]\n",
      "     * Parents: []\n",
      "     * Children: [\"TechnologyAdoptionRate\", \"AIInvestment\", \"IoTInfrastructureDevelopment\"]\n",
      "  b. Name: CulturalAcceptance\n",
      "     * Description: Level of cultural acceptance towards new technologies among different populations.\n",
      "     * Type: confounder\n",
      "     * Observed: Yes\n",
      "     * Values: [\"Resistant\", \"Embracing\"]\n",
      "     * Parents: []\n",
      "     * Children: [\"TechnologyAdoptionRate\", \"AIInvestment\", \"IoTInfrastructureDevelopment\"]\n",
      "4. Mediator Variables:\n",
      "  a. Name: PublicAwareness\n",
      "     * Description: Public awareness and understanding of the benefits and implications of certain technologies.\n",
      "     * Type: mediator\n",
      "     * Observable: Maybe\n",
      "     * Values: [\"Low\", \"High\"]\n",
      "     * Parents: [\"EconomicConditions\", \"CulturalAcceptance\"]\n",
      "     * Children: [\"TechnologyAdoptionRate\", \"AIInvestment\", \"IoTInfrastructureDevelopment\"]\n",
      "  b. Name: RegulatoryEnvironment\n",
      "     * Description: The regulatory landscape impacting technological innovation and its adoption.\n",
      "     * Type: mediator\n",
      "     * Observable: Maybe\n",
      "     * Values: [\"Favorable\", \"Unfavorable\"]\n",
      "     * Parents: [\"GovernmentPolicy\", \"PublicPerception\"]\n",
      "     * Children: [\"TechnologyAdoptionRate\", \"AIInvestment\", \"IoTInfrastructureDevelopment\"]\n",
      "5. Collider Variable:\n",
      "  * Name: MarketDemand\n",
      "     * Description: Overall market demand for new technologies, influenced by public perception, government policy, and socioeconomic factors.\n",
      "     * Type: collider\n",
      "     * Observed: Yes\n",
      "     * Values: [\"Low\", \"High\"]\n",
      "     * Parents: [\"PublicAwareness\", \"RegulatoryEnvironment\", \"CulturalAcceptance\"]\n",
      "Based on the given constraints, here's the JSON representation of the causal graph:\n",
      "[{\n",
      "   \"name\": \"TechnologyAdoptionRate\",\n",
      "   \"description\": \"The rate at which new technology is adopted globally.\",\n",
      "   \"type\": \"outcome\",\n",
      "   \"observed\": true,\n",
      "   \"values\": [\"Low\", \"Medium\", \"High\"],\n",
      "   \"parents\": []\n",
      "}, {\n",
      "   \"name\": \"AIInvestment\",\n",
      "   \"description\": \"Investment in artificial intelligence research and implementation.\",\n",
      "   \"type\": \"treatment\",\n",
      "   \"observed\": true,\n",
      "   \"values\": [\"Low\", \"High\"],\n",
      "   \"parents\": []\n",
      "}, {\n",
      "   \"name\": \"IoTInfrastructureDevelopment\",\n",
      "   \"description\": \"Development and expansion of Internet of Things infrastructure.\",\n",
      "   \"type\": \"treatment\",\n",
      "   \"observed\": true,\n",
      "   \"values\": [\"Limited\", \"Expanded\"],\n",
      "   \"parents\": []\n",
      "}, {\n",
      "   \"name негомогенетическая причина\",\n",
      "   \"description\": \"General economic conditions influencing investment decisions and consumer behavior.\",\n",
      "   \"type\": \"confounder\",\n",
      "   \"observed\": true,\n",
      "   \"values\": [\"Recessionary\", \"Prosperous\"],\n",
      "   \"parents\": [],\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, pad_token_id = tokenizer.eos_token_id)\n",
    "# print(outputs[0][\"generated_text\"])\n",
    "\n",
    "# prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "with torch.no_grad():\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=1024, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, repetition_penalty=1.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T16:15:12.589654557Z",
     "start_time": "2024-01-15T16:14:01.510296420Z"
    }
   },
   "id": "8f31d447667f849",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nodes = {\n",
    "    \n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50d5822772438f52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dot = Digraph(comment='Causal Model')\n",
    "node_styles = {\n",
    "    \"treatment\": {\"shape\": \"ellipse\", \"style\": \"filled\", \"color\": \"lightpink\"},\n",
    "    \"outcome\": {\"shape\": \"ellipse\", \"style\": \"filled\", \"color\": \"lightgreen\"},\n",
    "    \"confounder\": {\"shape\": \"diamond\", \"style\": \"filled\", \"color\": \"lightblue\"},\n",
    "    \"mediator\": {\"shape\": \"box\", \"style\": \"filled\", \"color\": \"lightyellow\"},\n",
    "    \"collider\": {\"shape\": \"box\", \"style\": \"filled\", \"color\": \"lightgray\"}\n",
    "}\n",
    "for node in ctx['nodes']:\n",
    "    style = node_styles.get(node[\"type\"], {\"shape\": \"ellipse\"})\n",
    "    dot.node(node[\"name\"], node[\"name\"], **style)\n",
    "for node in ctx['nodes']:\n",
    "    for parent in node[\"parents\"]:\n",
    "        dot.edge(parent, node[\"name\"])\n",
    "# dot.view()\n",
    "dot.render(str(misc.temp_root() / 'graph'), format='png')\n",
    "Image.open(misc.temp_root() / 'graph.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1153c7da52abdb5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f519159e30b75ab3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7062b6acba9976a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "60aa9cba4de17d0f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0ffd9372c828b1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = '''We have a news article headline, and we would like to identify a potential specific analysis that could be motivated by the article. Importantly, we would like specific details to be mentioned in the setting, even if they don't show up in the original article, to paint a vivid picture of the motivation and potential features of the data.\n",
    "\n",
    "Here is the article headline:\n",
    "\n",
    "```\n",
    "Title: Technological innovations that we will see in 2021 - 15 min\n",
    "Description: Artificial intelligence, virtual and augmented reality, the Internet of Things, robotics, automation... For almost five years, reviews of future technologies have been starting...\n",
    "Original Language: Lithuanian\n",
    "```\n",
    "\n",
    "Write a 2-3 sentence introduction to a statistical analysis that could be motivated by this article. Be sure to include specific details about the data that would be used and the analysis that would be performed. You do not need to write the entire analysis, just the introduction. Avoid using any technical jargon (such as words like \"statistical analysis\" or \"model\" and avoid starting with \"imagine\") and make sure the introduction is accessible to laypeople (albeit in a professional setting), and the motivation is clear. Imagine a casual conversation between friends or colleagues about the article that may prompt some deeper quantitative analysis when thinking about it, and make sure this introduction would sound understandable and interesting to them.\n",
    "\n",
    "Answer in the form of a json dictionary with two entries: one for the `introduction` and one for a proposed `title` for this specific setting that can serve as a file name.\n",
    "\n",
    "'''\n",
    "\n",
    "prompt = '''Beantworten Sie die folgende Frage: Wie viele Helikopter kann ein Mensch in einer Sitzung essen?\n",
    "\n",
    "Antwort:'''\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.to('cuda');\n",
    "N = inputs['input_ids'].shape[1]\n",
    "N"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T09:33:54.683854663Z",
     "start_time": "2024-01-15T09:33:54.604120660Z"
    }
   },
   "id": "f53e3eebe879dd11",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beantworten Sie die folgende Frage: Wie viele Helikopter kann ein Mensch in einer Sitzung essen?\n",
      "\n",
      "Antwort: Ein Mensch kann ein Helikopter in einer Sitzung essen, wenn sie eine wichtigste und einzige Wirkungsfälschung seine Wahl oder einer Wahlzuschlag zu verstehen.\n",
      "\n",
      "Diese Frage wird aufgrund der wichtigsten und einzige Wirkungsfälschung seiner Wahl oder\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=N+100)\n",
    "    completed = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(completed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T09:34:01.449958879Z",
     "start_time": "2024-01-15T09:33:58.093463949Z"
    }
   },
   "id": "4c67c1480c9ff4e9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T14:34:09.496121737Z",
     "start_time": "2024-01-15T14:34:07.546095654Z"
    }
   },
   "id": "80782da36efe4d19",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e456bd1a98a4993b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2182724902e46e295fb3e09c2f1757f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.20.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.0.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"microsoft/phi-2\", torch_dtype=torch.bfloat16, device_map=\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T14:34:48.557726398Z",
     "start_time": "2024-01-15T14:34:29.664847672Z"
    }
   },
   "id": "587d3b656c04cb30",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "533dddda775a5046"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "79b1fa4cbcddcf20"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Install transformers from source - only needed for versions <= v4.34\n",
    "# # pip install git+https://github.com/huggingface/transformers.git\n",
    "# # pip install accelerate\n",
    "# \n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# \n",
    "# pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd1dfa04e7eaa7c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# ...\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a80b3c738a7993"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1cae930b043d5734"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "model.to(device);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:59:14.610431822Z",
     "start_time": "2024-01-14T15:59:14.563110588Z"
    }
   },
   "id": "1c1c26b89ffa4b41",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'<s> My favourite condiment is the humble avocado. I just LOVE avocado and everything you can do with it. I love it for breakfast, snack or main meals. It makes any and everything nice with the nice creaminess and texture it has. I also love avocado because it is extremely versatile too. Yes, it is true and I am living proof because I have some avocado-focused menu planning posts so far in this blog with Avocado for Breakfast,'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T16:01:11.171953691Z",
     "start_time": "2024-01-14T15:59:30.206417593Z"
    }
   },
   "id": "13a2720d03f7e88e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd79c68464cda0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        quantize_config=None)\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.214245838Z",
     "start_time": "2023-08-23T09:24:03.157425520Z"
    }
   },
   "id": "3e6c310cd578acff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.287111569Z",
     "start_time": "2023-08-23T09:24:03.200967934Z"
    }
   },
   "id": "8b3298b4e25191e4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:36:17.905430865Z",
     "start_time": "2023-08-23T09:36:15.840639353Z"
    }
   },
   "id": "305c9c750f596ebd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.287395717Z",
     "start_time": "2023-08-23T09:24:03.201075287Z"
    }
   },
   "id": "482d174c29c15686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.287460291Z",
     "start_time": "2023-08-23T09:24:03.201099240Z"
    }
   },
   "id": "5a8232d5254da47e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bab3e53f1af7448e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
