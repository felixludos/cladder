{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.213647675Z",
     "start_time": "2023-08-23T09:24:03.154996176Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        quantize_config=None)\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.214245838Z",
     "start_time": "2023-08-23T09:24:03.157425520Z"
    }
   },
   "id": "3e6c310cd578acff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.287111569Z",
     "start_time": "2023-08-23T09:24:03.200967934Z"
    }
   },
   "id": "8b3298b4e25191e4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:36:17.905430865Z",
     "start_time": "2023-08-23T09:36:15.840639353Z"
    }
   },
   "id": "305c9c750f596ebd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.287395717Z",
     "start_time": "2023-08-23T09:24:03.201075287Z"
    }
   },
   "id": "482d174c29c15686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T09:24:03.287460291Z",
     "start_time": "2023-08-23T09:24:03.201099240Z"
    }
   },
   "id": "5a8232d5254da47e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bab3e53f1af7448e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
